import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Dense, Dropout, BatchNormalization,
                                     ReLU, ELU, Concatenate, Activation)
from tensorflow.keras.optimizers import  AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras import regularizers
from tensorflow.keras import backend as K

from bayes_opt import BayesianOptimization

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.family"] = ["SimHei", "SimSun", "Microsoft YaHei"]
plt.rcParams["axes.unicode_minus"] = False

# æ•°æ®è¯»å–ä¸é¢„å¤„ç†
df = pd.read_csv("Activity_standardized.csv")
numerical_cols = ['Cat(umol)', 'Al/M(molar)', 't/min', 'T/C', 'R1', 'R2']
one_hot_cols = ['M_Zr', 'M_Ti', 'M_Hf', 'R3']
target_col = 'Activity(KgPP/mol cat)'

X_num = df[numerical_cols].values
X_cat = df[one_hot_cols].values
y = df[target_col].values

X_num_train, X_num_temp, X_cat_train, X_cat_temp, y_train, y_temp = train_test_split(
    X_num, X_cat, y, test_size=0.3, random_state=42
)
X_num_val, X_num_test, X_cat_val, X_cat_test, y_val, y_test = train_test_split(
    X_num_temp, X_cat_temp, y_temp, test_size=1/3, random_state=42
)

# å›ºå®šéšæœºç§å­
def set_seed(seed):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

set_seed(42)

# è‡ªå®šä¹‰ RÂ² è¯„ä¼°å‡½æ•°
def r2_metric(y_true, y_pred):
    ss_res = K.sum(K.square(y_true - y_pred))
    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return 1 - ss_res / (ss_tot + K.epsilon())

# æ¿€æ´»å‡½æ•°æ˜ å°„è¡¨
activation_map = {
    'relu': ReLU(),
    'elu': ELU(),
    'swish': Activation(tf.nn.swish),
    'gelu': Activation(tf.nn.gelu)
}
activation_choices = list(activation_map.keys())

# è´å¶æ–¯ç›®æ ‡å‡½æ•°
def build_and_evaluate_model(units_num, units_cat, dropout_num, dropout_cat,
                             dropout_num_2, dropout_cat_2,
                             learning_rate, l2_reg, batch_size, act_choice_idx, fusion_dim):
    units_num = int(units_num)
    units_cat = int(units_cat)
    batch_size = int(batch_size)
    fusion_dim = int(fusion_dim)
    act_choice = activation_choices[int(act_choice_idx)]
    act_layer = activation_map[act_choice]

    input_num = Input(shape=(X_num_train.shape[1],))
    x_num = Dense(units_num, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=l2_reg))(input_num)
    x_num = BatchNormalization()(x_num)
    x_num = act_layer(x_num)
    x_num = Dropout(dropout_num)(x_num)
    x_num = Dense(units_num // 2)(x_num)
    x_num = act_layer(x_num)
    x_num = Dropout(dropout_num_2)(x_num)

    input_cat = Input(shape=(X_cat_train.shape[1],))
    x_cat = Dense(units_cat)(input_cat)
    x_cat = act_layer(x_cat)
    x_cat = Dropout(dropout_cat)(x_cat)
    x_cat = Dense(units_cat // 2)(x_cat)
    x_cat = act_layer(x_cat)
    x_cat = Dropout(dropout_cat_2)(x_cat)

    x = Concatenate()([x_num, x_cat])
    x = Dense(fusion_dim, activation='relu')(x)
    output = Dense(1)(x)

    model = Model(inputs=[input_num, input_cat], outputs=output)
    model.compile(optimizer=AdamW(learning_rate=learning_rate, weight_decay=l2_reg),
                  loss=tf.keras.losses.Huber(delta=1.0),
                  metrics=['mae', r2_metric])

    early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)
    model_checkpoint = ModelCheckpoint(
        filepath="best_model_weights.h5",
        monitor='val_loss',
        save_best_only=True,
        save_weights_only=True,
        verbose=0
    )

    model.fit([X_num_train, X_cat_train], y_train,
              validation_data=([X_num_val, X_cat_val], y_val),
              epochs=300,
              batch_size=batch_size,
              verbose=0,
              callbacks=[early_stop, reduce_lr, model_checkpoint])

    model.load_weights("best_model_weights.h5")

    y_val_pred = model.predict([X_num_val, X_cat_val]).flatten()
    val_mse = mean_squared_error(y_val, y_val_pred)
    val_r2 = r2_score(y_val, y_val_pred)

    score = 0.3 * val_mse + 0.7 * (1 - val_r2)
    return -score

pbounds = {
    'units_num': (64, 200),
    'units_cat': (64, 128),
    'fusion_dim': (64, 150),
    'dropout_num': (0.1, 0.2),
    'dropout_cat': (0.1, 0.35),
    'dropout_num_2': (0.1, 0.3),
    'dropout_cat_2': (0.1, 0.2),
    'learning_rate': (5e-5, 5e-4),     # âœ… é™ä½å­¦ä¹ ç‡èŒƒå›´
    'l2_reg': (1e-5, 1e-4),            # âœ… å¢å¼ºæ­£åˆ™
    'batch_size': (64, 100),           # âœ… ç¨å¾®å¢å¤§ batch size
    'act_choice_idx': (0, len(activation_choices) - 1)
}


optimizer = BayesianOptimization(
    f=build_and_evaluate_model,
    pbounds=pbounds,
    random_state=42,
    verbose=2
)

optimizer.maximize(init_points=15, n_iter=60)

print("\n\u2705 æœ€ä½³è¶…å‚æ•°é…ç½®ï¼š")
for k, v in optimizer.max['params'].items():
    if k == 'act_choice_idx':
        print(f"{k} = {activation_choices[int(v)]} (index={int(v)})")
    elif isinstance(v, float):
        print(f"{k} = {v}")
    else:
        print(f"{k} = {v}")

# ä½¿ç”¨æœ€ä½³å‚æ•°é‡æ–°è®­ç»ƒå¹¶è¯„ä¼°
best = optimizer.max['params']
best['units_num'] = int(best['units_num'])
best['units_cat'] = int(best['units_cat'])
best['batch_size'] = int(best['batch_size'])
best['fusion_dim'] = int(best['fusion_dim'])
best['act_choice_idx'] = int(best['act_choice_idx'])
best['act_choice'] = activation_choices[best['act_choice_idx']]
act_layer = activation_map[best['act_choice']]

input_num = Input(shape=(X_num.shape[1],))
x_num = Dense(best['units_num'], kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=best['l2_reg']))(input_num)
x_num = BatchNormalization()(x_num)
x_num = act_layer(x_num)
x_num = Dropout(best['dropout_num'])(x_num)
x_num = Dense(best['units_num'] // 2)(x_num)
x_num = act_layer(x_num)
x_num = Dropout(0.2)(x_num)


input_cat = Input(shape=(X_cat.shape[1],))
x_cat = Dense(best['units_cat'])(input_cat)
x_cat = act_layer(x_cat)
x_cat = Dropout(best['dropout_cat'])(x_cat)
x_cat = Dense(best['units_cat'] // 2)(x_cat)
x_cat = act_layer(x_cat)
x_cat = Dropout(0.2)(x_cat)


x = Concatenate()([x_num, x_cat])
x = Dense(best['fusion_dim'], activation='relu')(x)
output = Dense(1)(x)

model = Model(inputs=[input_num, input_cat], outputs=output)
model.compile(optimizer=AdamW(learning_rate=best['learning_rate'], weight_decay=best['l2_reg']),
              loss=tf.keras.losses.Huber(delta=1.0),
              metrics=['mae', r2_metric])

early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)
model_checkpoint = ModelCheckpoint(
    filepath="best_model_weights.h5",
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=True,
    verbose=1
)

history = model.fit([X_num_train, X_cat_train], y_train,
                    validation_data=([X_num_val, X_cat_val], y_val),
                    epochs=300,
                    batch_size=best['batch_size'],
                    verbose=1,
                    callbacks=[early_stop, reduce_lr, model_checkpoint])

# è®­ç»ƒåè‡ªåŠ¨åŠ è½½æœ€ä½³æƒé‡
model.load_weights("best_model_weights.h5")

# ==========================
# å­¦ä¹ æ›²çº¿å¯è§†åŒ–
# ==========================
history_dict = history.history

# ç»˜åˆ¶ RÂ² å­¦ä¹ æ›²çº¿
train_r2 = history_dict['r2_metric']
val_r2 = history_dict['val_r2_metric']

plt.figure(figsize=(12, 6))
plt.plot(train_r2, label='è®­ç»ƒé›† RÂ²', color='blue')
plt.plot(val_r2, label='éªŒè¯é›† RÂ²', color='orange')
plt.title('è®­ç»ƒé›†å’ŒéªŒè¯é›† RÂ² éšè®­ç»ƒæ¬¡æ•°(epoch)çš„å˜åŒ–')
plt.xlabel('è®­ç»ƒæ¬¡æ•°(epoch)')
plt.ylabel('RÂ²')
plt.legend()
plt.grid(True)
plt.show()

# ç»˜åˆ¶ MSE å­¦ä¹ æ›²çº¿
train_mse = history_dict['loss']
val_mse = history_dict['val_loss']
plt.figure(figsize=(12, 6))
plt.plot(train_mse, label='è®­ç»ƒé›† MSE', color='blue')
plt.plot(val_mse, label='éªŒè¯é›† MSE', color='orange')
plt.title('è®­ç»ƒé›†å’ŒéªŒè¯é›† MSE éšè®­ç»ƒæ¬¡æ•°(epoch)çš„å˜åŒ–')
plt.xlabel('è®­ç»ƒæ¬¡æ•°(epoch)')
plt.ylabel('MSE')
plt.legend()
plt.grid(True)
plt.show()


# ========================== åŒçºµåæ ‡è½´å­¦ä¹ æ›²çº¿ï¼ˆRÂ² + MSEï¼‰ ==========================
history_dict = history.history

train_r2 = history_dict['r2_metric']
val_r2 = history_dict['val_r2_metric']
train_mse = history_dict['loss']
val_mse = history_dict['val_loss']
epochs = range(1, len(train_r2) + 1)

fig, ax1 = plt.subplots(figsize=(12, 6))

# è®¾ç½®ç»Ÿä¸€æ ·å¼å‚æ•°
common_kwargs = dict(marker='o', linestyle='--', linewidth=1.5, markersize=5)

# å·¦è½´ï¼šMSE
ax1.set_xlabel('è®­ç»ƒæ¬¡æ•° (epoch)')
ax1.set_ylabel('å‡æ–¹è¯¯å·® (MSE)', color='tab:blue')
l1 = ax1.plot(epochs, train_mse, color='tab:blue', label='è®­ç»ƒ MSE', **common_kwargs)
l2 = ax1.plot(epochs, val_mse, color='tab:cyan', label='éªŒè¯ MSE', **common_kwargs)
ax1.tick_params(axis='y', labelcolor='tab:blue')

# å³è½´ï¼šRÂ²
ax2 = ax1.twinx()
ax2.set_ylabel('å†³å®šç³»æ•° RÂ²', color='tab:red')
l3 = ax2.plot(epochs, train_r2, color='tab:red', label='è®­ç»ƒ RÂ²', **common_kwargs)
l4 = ax2.plot(epochs, val_r2, color='tab:orange', label='éªŒè¯ RÂ²', **common_kwargs)
ax2.tick_params(axis='y', labelcolor='tab:red')
ax2.set_ylim(0, 1.05)
ax2.set_yticks(np.arange(0, 1.1, 0.1))

# åˆå¹¶å›¾ä¾‹
lines = l1 + l2 + l3 + l4
labels = [line.get_label() for line in lines]
fig.legend(lines, labels, loc='lower center', ncol=4)

plt.title('è®­ç»ƒè¿‡ç¨‹å­¦ä¹ æ›²çº¿ï¼ˆMSE + RÂ²ï¼‰')
plt.grid(True)
plt.tight_layout(rect=[0, 0.05, 1, 1])
plt.show()


# ========================== åŸæœ‰çš„æµ‹è¯•é›†é¢„æµ‹å¯¼å‡ºä»£ç ï¼ˆä¿æŒä¸å˜ï¼‰ ==========================
y_pred_test = model.predict([X_num_test, X_cat_test]).flatten()

test_result = pd.DataFrame({
    'çœŸå®å€¼': y_test,
    'é¢„æµ‹å€¼': y_pred_test
})
test_result.to_excel(r'F:/test_pred_vs_true.xlsx', index=False)
print("âœ… å·²å°†æµ‹è¯•é›†çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¿å­˜åˆ° F:/test_pred_vs_true.xlsx")

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_test, c='dodgerblue', alpha=0.7, edgecolors='k', label='é¢„æµ‹ç‚¹')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹çº¿')
plt.xlabel('çœŸå®å€¼', fontsize=12)
plt.ylabel('é¢„æµ‹å€¼', fontsize=12)
plt.title('æµ‹è¯•é›†ï¼šçœŸå®å€¼ vs é¢„æµ‹å€¼', fontsize=14)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ========================== éªŒè¯é›†è¯„ä¼°ï¼ˆä¿æŒä¸å˜ï¼‰ ==========================
y_pred_val = model.predict([X_num_val, X_cat_val]).flatten()

mse = mean_squared_error(y_val, y_pred_val)
mae = mean_absolute_error(y_val, y_pred_val)
r2 = r2_score(y_val, y_pred_val)

print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœï¼ˆéªŒè¯é›†ï¼‰ï¼š")
print(f"MSE: {mse}")
print(f"MAE: {mae}")
print(f"RÂ²: {r2}")

# ========================== âœ… æ–°å¢ï¼šæµ‹è¯•é›†è¯„ä¼° ==========================
mse_test = mean_squared_error(y_test, y_pred_test)
mae_test = mean_absolute_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)

print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœï¼ˆæµ‹è¯•é›†ï¼‰ï¼š")
print(f"æµ‹è¯•é›† MSE: {mse_test}")
print(f"æµ‹è¯•é›† MAE: {mae_test}")
print(f"æµ‹è¯•é›† RÂ²: {r2_test}")
