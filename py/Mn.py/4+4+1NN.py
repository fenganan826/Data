import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import (Input, Dense, Dropout, BatchNormalization,
                                     ReLU, ELU, Concatenate, Activation)
from tensorflow.keras.optimizers import AdamW
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers
from tensorflow.keras import backend as K

from bayes_opt import BayesianOptimization

# ====================== å…¨å±€é…ç½® ======================
plt.rcParams["font.family"] = ["SimHei", "SimSun", "Microsoft YaHei"]
plt.rcParams["axes.unicode_minus"] = False

def set_seed(seed=42):
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

set_seed(42)

# ====================== æ•°æ®è¯»å–ä¸é¢„å¤„ç† ======================
file_path = r'F:/Mn_standardized.xlsx'
df = pd.read_excel(file_path)

numerical_cols = ['Cat(umol)', 'Al/M(molar)', 't/min', 'T/â„ƒ', 'R1', 'R2']
one_hot_cols = ['M_Zr', 'M_Ti', 'M_Hf', 'R3']
target_col = 'Mn(Ã—104g/molï¼‰'

X_num = df[numerical_cols].values
X_cat = df[one_hot_cols].values
y = df[target_col].values

# 8:2 åˆ’åˆ†ï¼ˆè®­ç»ƒéªŒè¯æ€»ä½“ trainval vs æµ‹è¯•é›† testï¼‰
X_num_trainval, X_num_test, X_cat_trainval, X_cat_test, y_trainval, y_test = train_test_split(
    X_num, X_cat, y, test_size=0.2, random_state=42
)

# ====================== è‡ªå®šä¹‰ RÂ² æŒ‡æ ‡ ======================
def r2_metric(y_true, y_pred):
    ss_res = K.sum(K.square(y_true - y_pred))
    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))
    return 1 - ss_res / (ss_tot + K.epsilon())

# ====================== æ¿€æ´»å‡½æ•°æ˜ å°„ï¼ˆä¸å‚è€ƒä»£ç ä¸€è‡´ï¼‰ ======================
activation_map = {
    'relu': ReLU(),
    'elu': ELU(),
    'swish': Activation(tf.nn.swish),
    'gelu': Activation(tf.nn.gelu)
}
activation_choices = list(activation_map.keys())

# ====================== è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°ï¼ˆ10 æŠ˜ï¼›é€æŠ˜æ‰“å°ï¼›ç»“æ„=å››å±‚ï¼‰ ======================
def build_and_evaluate_model_cv(units_num, units_cat,
                                dropout_num1, dropout_num2, dropout_num3, dropout_num4,
                                dropout_cat1, dropout_cat2, dropout_cat3, dropout_cat4,
                                learning_rate, l2_reg, batch_size,
                                act_choice_idx, fusion_dim):

    # åŸºæœ¬ç±»å‹è½¬æ¢
    units_num = int(units_num)
    units_cat = int(units_cat)
    batch_size = int(batch_size)
    fusion_dim = int(fusion_dim)
    act_choice = activation_choices[int(act_choice_idx)]
    act_layer = activation_map[act_choice]

    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    val_scores = []

    print("\n================== å¼€å§‹ 10 æŠ˜äº¤å‰éªŒè¯ï¼ˆç”¨äºè´å¶æ–¯ä¼˜åŒ–ï¼‰ ==================")
    print(f"æ¿€æ´»: {act_choice} | lr={learning_rate:.6g} | l2={l2_reg:.6g} | "
          f"units_num={units_num} | units_cat={units_cat} | fusion_dim={fusion_dim} | batch_size={batch_size}")
    print("=====================================================================\n")

    for fold, (train_idx, val_idx) in enumerate(kf.split(X_num_trainval), start=1):
        Xn_train, Xn_val = X_num_trainval[train_idx], X_num_trainval[val_idx]
        Xc_train, Xc_val = X_cat_trainval[train_idx], X_cat_trainval[val_idx]
        y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]

        # ===== æ•°å€¼åˆ†æ”¯ï¼šå››å±‚ Dense + å››æ¬¡ Dropoutï¼ˆé¦–å±‚å« BNï¼‰ =====
        input_num = Input(shape=(Xn_train.shape[1],))
        x_num = Dense(units_num, kernel_regularizer=regularizers.l1_l2(1e-5, l2_reg))(input_num)
        x_num = BatchNormalization()(x_num)
        x_num = act_layer(x_num)
        x_num = Dropout(dropout_num1)(x_num)

        x_num = Dense(units_num)(x_num)
        x_num = act_layer(x_num)
        x_num = Dropout(dropout_num2)(x_num)

        x_num = Dense(units_num // 2)(x_num)
        x_num = act_layer(x_num)
        x_num = Dropout(dropout_num3)(x_num)

        x_num = Dense(units_num // 4)(x_num)
        x_num = act_layer(x_num)
        x_num = Dropout(dropout_num4)(x_num)

        # ===== ç±»åˆ«åˆ†æ”¯ï¼šå››å±‚ Dense + å››æ¬¡ Dropout =====
        input_cat = Input(shape=(Xc_train.shape[1],))
        x_cat = Dense(units_cat)(input_cat)
        x_cat = act_layer(x_cat)
        x_cat = Dropout(dropout_cat1)(x_cat)

        x_cat = Dense(units_cat)(x_cat)
        x_cat = act_layer(x_cat)
        x_cat = Dropout(dropout_cat2)(x_cat)

        x_cat = Dense(units_cat // 2)(x_cat)
        x_cat = act_layer(x_cat)
        x_cat = Dropout(dropout_cat3)(x_cat)

        x_cat = Dense(units_cat // 4)(x_cat)
        x_cat = act_layer(x_cat)
        x_cat = Dropout(dropout_cat4)(x_cat)

        # èåˆ + è¾“å‡º
        x = Concatenate()([x_num, x_cat])
        x = Dense(fusion_dim, activation='relu')(x)
        output = Dense(1)(x)

        model = Model(inputs=[input_num, input_cat], outputs=output)

        optimizer = AdamW(learning_rate=learning_rate, weight_decay=l2_reg)
        model.compile(optimizer=optimizer,
                      loss=tf.keras.losses.Huber(delta=1.0),  # BO é˜¶æ®µä½¿ç”¨ Huberï¼ˆæŠ—å¼‚å¸¸å€¼ï¼‰
                      metrics=['mae', r2_metric])

        callbacks = [
            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)
        ]

        print(f"â–¶ï¸ æŠ˜ {fold:02d} å¼€å§‹è®­ç»ƒ...")
        history = model.fit([Xn_train, Xc_train], y_train,
                            validation_data=([Xn_val, Xc_val], y_val),
                            epochs=200,
                            batch_size=batch_size,
                            verbose=0,
                            callbacks=callbacks)

        # â€”â€” æ‰“å°æ¯æŠ˜æœ€åä¸€è½®çš„è®­ç»ƒ/éªŒè¯æŒ‡æ ‡ï¼ˆæ³¨æ„ï¼šloss=Huberï¼‰ â€”â€”
        tr_r2_last   = history.history['r2_metric'][-1]
        va_r2_last   = history.history['val_r2_metric'][-1]
        tr_loss_last = history.history['loss'][-1]
        va_loss_last = history.history['val_loss'][-1]
        print(f"æŠ˜ {fold:02d} è®­ç»ƒé›† RÂ²: {tr_r2_last:.6f} | éªŒè¯é›† RÂ²: {va_r2_last:.6f}")
        print(f"æŠ˜ {fold:02d} è®­ç»ƒé›† Huber: {tr_loss_last:.6f} | éªŒè¯é›† Huber: {va_loss_last:.6f}")

        # â€”â€” sklearn è¯„ä¼°éªŒè¯é›†ï¼šMSE/RÂ²ï¼ˆç”¨äºç»¼åˆåˆ†æ•°ï¼‰ â€”â€”
        y_val_pred = model.predict([Xn_val, Xc_val], verbose=0).flatten()
        mse = mean_squared_error(y_val, y_val_pred)
        r2 = r2_score(y_val, y_val_pred)
        score = 0.3 * mse + 0.7 * (1 - r2)  # è¶Šå°è¶Šå¥½
        val_scores.append(score)

        print(f"æŠ˜ {fold:02d} éªŒè¯é›† MSE: {mse:.6f} | éªŒè¯é›† RÂ²: {r2:.6f} | ç»¼åˆåˆ†æ•°(0.3*MSE+0.7*(1-RÂ²)): {score:.6f}\n")

    mean_score = np.mean(val_scores)
    print("============== 10 æŠ˜äº¤å‰éªŒè¯å®Œæˆ ==============")
    print(f"10 æŠ˜éªŒè¯ç»¼åˆåˆ†æ•°å‡å€¼ï¼ˆè¶Šå°è¶Šå¥½ï¼‰: {mean_score:.6f}")
    print("ï¼ˆæ³¨ï¼šè´å¶æ–¯ä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å¤§åŒ–ï¼Œå› æ­¤ä¼šå¯¹è¯¥å‡å€¼å–è´Ÿï¼‰")
    print("=============================================\n")

    # BO ç›®æ ‡å‡½æ•°è¦æœ€å¤§åŒ–ï¼Œè¿”å›è´Ÿå·
    return -mean_score

# ====================== è´å¶æ–¯ä¼˜åŒ–å‚æ•°ç©ºé—´ï¼ˆå«å››ä¸ª dropoutï¼‰ ======================
pbounds = {
    'units_num': (32, 128),
    'units_cat': (32, 156),
    'fusion_dim': (64, 128),
    'dropout_num1': (0.15, 0.2),
    'dropout_num2': (0.1, 0.2),
    'dropout_num3': (0.1, 0.3),
    'dropout_num4': (0.1, 0.3),
    'dropout_cat1': (0.1, 0.3),
    'dropout_cat2': (0.15, 0.3),
    'dropout_cat3': (0.1, 0.3),
    'dropout_cat4': (0.1, 0.3),
    'learning_rate': (3e-4, 8e-4),
    'l2_reg': (5e-6, 5e-5),
    'batch_size': (32, 64),
    'act_choice_idx': (0, len(activation_choices) - 1)
}

optimizer = BayesianOptimization(
    f=build_and_evaluate_model_cv,
    pbounds=pbounds,
    random_state=42,
    verbose=2
)

optimizer.maximize(init_points=5, n_iter=10)

best = optimizer.max['params']
for key in ['units_num', 'units_cat', 'fusion_dim', 'batch_size', 'act_choice_idx']:
    best[key] = int(best[key])
best['act_choice'] = activation_choices[best['act_choice_idx']]

print("\nğŸ“Œ æœ€ä½³è¶…å‚æ•°ï¼š")
for k, v in best.items():
    print(f"{k}: {v}")

# ====================== æ„å»ºäº¤å‰éªŒè¯å­¦ä¹ æ›²çº¿ï¼ˆ10 æŠ˜å‡å€¼ï¼›æŸå¤±=MSEï¼›ç»“æ„=å››å±‚ï¼‰ ======================
kf = KFold(n_splits=10, shuffle=True, random_state=42)
num_epochs = 200  # ç»Ÿä¸€ä¸º200

train_mse_all = np.zeros((num_epochs, 10))
val_mse_all = np.zeros((num_epochs, 10))
train_r2_all = np.zeros((num_epochs, 10))
val_r2_all = np.zeros((num_epochs, 10))

for fold, (train_idx, val_idx) in enumerate(kf.split(X_num_trainval), start=1):
    Xn_train, Xn_val = X_num_trainval[train_idx], X_num_trainval[val_idx]
    Xc_train, Xc_val = X_cat_trainval[train_idx], X_cat_trainval[val_idx]
    y_train, y_val = y_trainval[train_idx], y_trainval[val_idx]

    act_layer = activation_map[best['act_choice']]

    # æ•°å€¼åˆ†æ”¯ï¼šå››å±‚
    input_num = Input(shape=(Xn_train.shape[1],))
    x_num = Dense(best['units_num'], kernel_regularizer=regularizers.l1_l2(1e-5, best['l2_reg']))(input_num)
    x_num = BatchNormalization()(x_num)
    x_num = act_layer(x_num)
    x_num = Dropout(best['dropout_num1'])(x_num)
    x_num = Dense(best['units_num'])(x_num)
    x_num = act_layer(x_num)
    x_num = Dropout(best['dropout_num2'])(x_num)
    x_num = Dense(best['units_num'] // 2)(x_num)
    x_num = act_layer(x_num)
    x_num = Dropout(best['dropout_num3'])(x_num)
    x_num = Dense(best['units_num'] // 4)(x_num)
    x_num = act_layer(x_num)
    x_num = Dropout(best['dropout_num4'])(x_num)

    # ç±»åˆ«åˆ†æ”¯ï¼šå››å±‚
    input_cat = Input(shape=(Xc_train.shape[1],))
    x_cat = Dense(best['units_cat'])(input_cat)
    x_cat = act_layer(x_cat)
    x_cat = Dropout(best['dropout_cat1'])(x_cat)
    x_cat = Dense(best['units_cat'])(x_cat)
    x_cat = act_layer(x_cat)
    x_cat = Dropout(best['dropout_cat2'])(x_cat)
    x_cat = Dense(best['units_cat'] // 2)(x_cat)
    x_cat = act_layer(x_cat)
    x_cat = Dropout(best['dropout_cat3'])(x_cat)
    x_cat = Dense(best['units_cat'] // 4)(x_cat)
    x_cat = act_layer(x_cat)
    x_cat = Dropout(best['dropout_cat4'])(x_cat)

    x = Concatenate()([x_num, x_cat])
    x = Dense(best['fusion_dim'], activation='relu')(x)
    output = Dense(1)(x)

    model = Model(inputs=[input_num, input_cat], outputs=output)

    optimizer_final = AdamW(learning_rate=best['learning_rate'], weight_decay=best['l2_reg'])
    # å­¦ä¹ æ›²çº¿é˜¶æ®µï¼šæ˜¾å¼ä½¿ç”¨ MSE ä½œä¸ºæŸå¤±
    model.compile(optimizer=optimizer_final, loss='mse', metrics=[r2_metric])

    history = model.fit([Xn_train, Xc_train], y_train,
                        validation_data=([Xn_val, Xc_val], y_val),
                        epochs=num_epochs,
                        batch_size=best['batch_size'],
                        verbose=0)

    train_mse_all[:, fold-1] = history.history['loss']       # MSE
    val_mse_all[:, fold-1]  = history.history['val_loss']    # MSE
    train_r2_all[:, fold-1] = history.history['r2_metric']   # RÂ²
    val_r2_all[:, fold-1]   = history.history['val_r2_metric']

# ====================== å­¦ä¹ æ›²çº¿ç»˜å›¾ï¼ˆæ”¹ä¸ºå‚è€ƒä»£ç çš„é£æ ¼ï¼›ä¸æ”¹æ€è·¯ï¼‰ ======================
mean_train_mse = train_mse_all.mean(axis=1)
mean_val_mse = val_mse_all.mean(axis=1)
mean_train_r2 = train_r2_all.mean(axis=1)
mean_val_r2 = val_r2_all.mean(axis=1)

epochs = range(1, num_epochs + 1)

fig, ax1 = plt.subplots(figsize=(12, 6))

# å·¦è½´ï¼šMSEï¼ˆå‚è€ƒé£æ ¼ï¼‰
ax1.set_xlabel('è®­ç»ƒæ¬¡æ•° (epoch)')
ax1.set_ylabel('å‡æ–¹è¯¯å·® (MSE)', color='tab:blue')
l1 = ax1.plot(epochs, mean_train_mse, 'o-', label='è®­ç»ƒ MSE', color='tab:blue')
l2 = ax1.plot(epochs, mean_val_mse, 's-', label='éªŒè¯ MSE', color='tab:cyan')
ax1.tick_params(axis='y', labelcolor='tab:blue')

# å³è½´ï¼šRÂ²ï¼ˆå‚è€ƒé£æ ¼ï¼‰
ax2 = ax1.twinx()
ax2.set_ylabel('å†³å®šç³»æ•° RÂ²', color='tab:red')
l3 = ax2.plot(epochs, mean_train_r2, 'o--', label='è®­ç»ƒ RÂ²', color='tab:red')
l4 = ax2.plot(epochs, mean_val_r2, 's--', label='éªŒè¯ RÂ²', color='tab:orange')
ax2.tick_params(axis='y', labelcolor='tab:red')
ax2.set_ylim(0, 1.05)
ax2.set_yticks(np.arange(0, 1.1, 0.1))

# åˆå¹¶å›¾ä¾‹ï¼ˆå‚è€ƒé£æ ¼ï¼‰
lines = l1 + l2 + l3 + l4
labels = [line.get_label() for line in lines]
fig.legend(lines, labels, loc='lower center', ncol=4)

plt.title('è®­ç»ƒè¿‡ç¨‹å­¦ä¹ æ›²çº¿ï¼ˆMSE + RÂ²ï¼‰')
plt.grid(True)
plt.tight_layout(rect=[0, 0.05, 1, 1])
plt.show()

# ====================== æœ€ç»ˆæ¨¡å‹ä¸æµ‹è¯•è¯„ä¼°ï¼ˆç”¨æ•´ä¸ª trainvalï¼›æŸå¤±=Huberï¼‰ ======================
act_layer = activation_map[best['act_choice']]

input_num = Input(shape=(X_num.shape[1],))
x_num = Dense(best['units_num'], kernel_regularizer=regularizers.l1_l2(1e-5, best['l2_reg']))(input_num)
x_num = BatchNormalization()(x_num)
x_num = act_layer(x_num)
x_num = Dropout(best['dropout_num1'])(x_num)
x_num = Dense(best['units_num'])(x_num)
x_num = act_layer(x_num)
x_num = Dropout(best['dropout_num2'])(x_num)
x_num = Dense(best['units_num'] // 2)(x_num)
x_num = act_layer(x_num)
x_num = Dropout(best['dropout_num3'])(x_num)
x_num = Dense(best['units_num'] // 4)(x_num)
x_num = act_layer(x_num)
x_num = Dropout(best['dropout_num4'])(x_num)

input_cat = Input(shape=(X_cat.shape[1],))
x_cat = Dense(best['units_cat'])(input_cat)
x_cat = act_layer(x_cat)
x_cat = Dropout(best['dropout_cat1'])(x_cat)
x_cat = Dense(best['units_cat'])(x_cat)
x_cat = act_layer(x_cat)
x_cat = Dropout(best['dropout_cat2'])(x_cat)
x_cat = Dense(best['units_cat'] // 2)(x_cat)
x_cat = act_layer(x_cat)
x_cat = Dropout(best['dropout_cat3'])(x_cat)
x_cat = Dense(best['units_cat'] // 4)(x_cat)
x_cat = act_layer(x_cat)
x_cat = Dropout(best['dropout_cat4'])(x_cat)

x = Concatenate()([x_num, x_cat])
x = Dense(best['fusion_dim'], activation='relu')(x)
output = Dense(1)(x)

final_model = Model(inputs=[input_num, input_cat], outputs=output)

optimizer_final = AdamW(learning_rate=best['learning_rate'], weight_decay=best['l2_reg'])
# ä¸ BO é˜¶æ®µä¿æŒä¸€è‡´ï¼šä½¿ç”¨ Huberï¼ˆæ›´æŠ—å¼‚å¸¸å€¼ï¼‰ï¼›å¦‚éœ€ä¸å­¦ä¹ æ›²çº¿ä¸€è‡´å¯æ”¹ 'mse'
final_model.compile(optimizer=optimizer_final,
                    loss=tf.keras.losses.Huber(delta=1.0),
                    metrics=['mae', r2_metric])

callbacks_final = [
    EarlyStopping(monitor='loss', patience=10, restore_best_weights=True),
    ReduceLROnPlateau(monitor='loss', factor=0.5, patience=8, min_lr=1e-6)
]

history_final = final_model.fit([X_num_trainval, X_cat_trainval], y_trainval,
                                epochs=200,
                                batch_size=best['batch_size'],
                                verbose=1,
                                callbacks=callbacks_final)

# ====================== è®­ç»ƒé›†ä¸æµ‹è¯•é›†çš„è¯„ä¼° ======================

# å¯¹è®­ç»ƒé›†è¿›è¡Œè¯„ä¼°
y_trainval_pred = final_model.predict([X_num_trainval, X_cat_trainval], verbose=0).flatten()
mse_trainval = mean_squared_error(y_trainval, y_trainval_pred)
mae_trainval = mean_absolute_error(y_trainval, y_trainval_pred)
r2_trainval = r2_score(y_trainval, y_trainval_pred)

print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœï¼ˆè®­ç»ƒé›†ï¼‰ï¼š")
print(f"è®­ç»ƒé›† MSE: {mse_trainval}")
print(f"è®­ç»ƒé›† MAE: {mae_trainval}")
print(f"è®­ç»ƒé›† RÂ²: {r2_trainval}")

# å¯¹æµ‹è¯•é›†è¿›è¡Œè¯„ä¼°
y_pred_test = final_model.predict([X_num_test, X_cat_test], verbose=0).flatten()

mse_test = mean_squared_error(y_test, y_pred_test)
mae_test = mean_absolute_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)

print("\nğŸ“Š æœ€ç»ˆæ¨¡å‹è¯„ä¼°ç»“æœï¼ˆæµ‹è¯•é›†ï¼‰ï¼š")
print(f"æµ‹è¯•é›† MSE: {mse_test}")
print(f"æµ‹è¯•é›† MAE: {mae_test}")
print(f"æµ‹è¯•é›† RÂ²: {r2_test}")

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_test, c='dodgerblue', alpha=0.7, edgecolors='k', label='é¢„æµ‹ç‚¹')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='ç†æƒ³é¢„æµ‹çº¿')
plt.xlabel('çœŸå®å€¼', fontsize=12)
plt.ylabel('é¢„æµ‹å€¼', fontsize=12)
plt.title('æµ‹è¯•é›†ï¼šçœŸå®å€¼ vs é¢„æµ‹å€¼', fontsize=14)
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# ä¿å­˜é¢„æµ‹ç»“æœ
save_path = r'F:/test_pred_vs_true.xlsx'
pd.DataFrame({'çœŸå®å€¼': y_test, 'é¢„æµ‹å€¼': y_pred_test}).to_excel(save_path, index=False)
print(f"âœ… å·²å°†æµ‹è¯•é›†çœŸå®å€¼ä¸é¢„æµ‹å€¼ä¿å­˜åˆ° {save_path}")
