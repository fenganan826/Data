import pandas as pd
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from bayes_opt import BayesianOptimization
import numpy as np
import matplotlib.pyplot as plt
import shap

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams["font.family"] = ["SimHei", "SimSun", "Microsoft YaHei"]
plt.rcParams["axes.unicode_minus"] = False

# è¯»å– Excel æ–‡ä»¶
file_path = r'F:/Mn_standardized.xlsx'
df = pd.read_excel(file_path)

# ç¡®å®šç‰¹å¾å’Œç›®æ ‡å˜é‡
X = df.drop('Mn(Ã—104g/molï¼‰', axis=1)
y = df['Mn(Ã—104g/molï¼‰']

# åŒºåˆ†ç±»åˆ«å‹å’Œæ•°å€¼å‹ç‰¹å¾ï¼ˆä¿æŒä¸å˜ï¼‰
categorical_cols = ['M_Zr', 'M_Hf', 'M_Ti', 'R3']
numeric_cols = [col for col in X.columns if col not in categorical_cols]

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ80% è®­ç»ƒï¼Œ20% æµ‹è¯•ï¼‰
X_trainval, X_test, y_trainval, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# åˆ›å»ºé¢„å¤„ç†æµæ°´çº¿ï¼ˆä¿æŒä¸å˜ï¼‰
preprocessor = ColumnTransformer(
    transformers=[
        ('num', 'passthrough', numeric_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

# ç›®æ ‡å‡½æ•°ï¼šåœ¨è®­ç»ƒé›†ä¸Šåš 10 æŠ˜äº¤å‰éªŒè¯ï¼ˆè´å¶æ–¯ä¼˜åŒ–ä½¿ç”¨ï¼‰
def objective(n_estimators, learning_rate, max_depth):
    n_estimators = int(n_estimators)
    max_depth = int(max_depth)  # ä¿ç•™è¯¥å‚æ•°ï¼Œä½†ä¸æ”¹åŠ¨ä½ çš„åŸæ¨¡å‹ï¼ˆAdaBoost é»˜è®¤å¼±å­¦ä¹ å™¨ï¼‰

    kf = KFold(n_splits=10, shuffle=True, random_state=42)
    fold_mses = []
    fold_train_r2_scores = []
    fold_val_r2_scores = []

    for tr_idx, va_idx in kf.split(X_trainval):
        X_tr = X_trainval.iloc[tr_idx]
        y_tr = y_trainval.iloc[tr_idx]
        X_va = X_trainval.iloc[va_idx]
        y_va = y_trainval.iloc[va_idx]

        pipeline = Pipeline(steps=[
            ('preprocessor', preprocessor),
            ('regressor', AdaBoostRegressor(
                n_estimators=n_estimators,
                learning_rate=learning_rate,
                random_state=42
            ))
        ])
        pipeline.fit(X_tr, y_tr)
        y_pred = pipeline.predict(X_va)

        fold_mses.append(mean_squared_error(y_va, y_pred))
        fold_train_r2_scores.append(r2_score(y_tr, pipeline.predict(X_tr)))
        fold_val_r2_scores.append(r2_score(y_va, y_pred))

        # è¾“å‡ºæ¯æŠ˜è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç»“æœ
        print(f"Fold {len(fold_mses)} è®­ç»ƒé›† MSE: {fold_mses[-1]}, è®­ç»ƒé›† RÂ²: {fold_train_r2_scores[-1]}")
        print(
            f"Fold {len(fold_mses)} éªŒè¯é›† MSE: {mean_squared_error(y_va, y_pred)}, éªŒè¯é›† RÂ²: {fold_val_r2_scores[-1]}")

    # è´å¶æ–¯ä¼˜åŒ–æœ€å¤§åŒ–ç›®æ ‡ â†’ è¿”å› -MSE çš„å‡å€¼
    return -float(np.mean(fold_mses))

# è´å¶æ–¯ä¼˜åŒ–å‚æ•°èŒƒå›´
pbounds = {
    'n_estimators': (32, 128),
    'learning_rate': (0.01, 0.2),
    'max_depth': (3, 10)  # ä¿ç•™ä½†ä¸æ”¹å˜æ¨¡å‹ç»“æ„
}

# æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–ï¼ˆåŸºäº 10 æŠ˜ CVï¼‰
optimizer = BayesianOptimization(
    f=objective,
    pbounds=pbounds,
    random_state=42,
)
optimizer.maximize(init_points=5, n_iter=10)

# è·å–æœ€ä½³å‚æ•°
best_params = optimizer.max['params']
best_n_estimators = int(best_params['n_estimators'])
best_learning_rate = best_params['learning_rate']
best_max_depth = int(best_params['max_depth'])

# è¾“å‡ºæœ€ä¼˜å‚æ•°ç»“æœ
print("\nğŸ“Œ è´å¶æ–¯ä¼˜åŒ–å¾—åˆ°çš„æœ€ä½³è¶…å‚æ•°ï¼š")
print(f"æœ€ä¼˜ n_estimators: {best_n_estimators}")
print(f"æœ€ä¼˜ learning_rate: {best_learning_rate}")
print(f"æœ€ä¼˜ max_depth: {best_max_depth}")

# æ„å»ºæœ€ç»ˆæ¨¡å‹ï¼ˆåœ¨æ•´ä¸ªè®­ç»ƒé›†ä¸Šè®­ç»ƒï¼‰
final_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', AdaBoostRegressor(
        n_estimators=best_n_estimators,
        learning_rate=best_learning_rate,
        random_state=42
    ))
])

final_pipeline.fit(X_trainval, y_trainval)

# ====== æ¨¡å‹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¯„ä¼° ======
# åœ¨è®­ç»ƒé›†ä¸Šçš„è¯„ä¼°
y_pred_train = final_pipeline.predict(X_trainval)
rmse_train = np.sqrt(mean_squared_error(y_trainval, y_pred_train))
r2_train = r2_score(y_trainval, y_pred_train)
mae_train = mean_absolute_error(y_trainval, y_pred_train)

print("\nâœ… è®­ç»ƒé›†æ¨¡å‹è¯„ä¼°ï¼š")
print(f"è®­ç»ƒé›†å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse_train}")
print(f"è®­ç»ƒé›† RÂ² åˆ†æ•°: {r2_train}")
print(f"è®­ç»ƒé›†å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_train}")

# åœ¨æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°
y_pred_test = final_pipeline.predict(X_test)
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
r2_test = r2_score(y_test, y_pred_test)
mae_test = mean_absolute_error(y_test, y_pred_test)

print("\nâœ… æµ‹è¯•é›†æ¨¡å‹è¯„ä¼°ï¼š")
print(f"æµ‹è¯•é›†å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse_test}")
print(f"æµ‹è¯•é›† RÂ² åˆ†æ•°: {r2_test}")
print(f"æµ‹è¯•é›†å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae_test}")

# ========== å­¦ä¹ æ›²çº¿ï¼šMSE + RÂ² ==========
# ä½¿ç”¨KFoldè¿›è¡Œäº¤å‰éªŒè¯çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ’åˆ†
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# ç”¨äºå­˜å‚¨æ¯ä¸€æŠ˜çš„è®­ç»ƒå’ŒéªŒè¯è¯¯å·®ï¼ˆåœ¨æ¯ä¸ª n_estimators ä¸‹ï¼‰
train_mse_folds = []
val_mse_folds = []
train_r2_folds = []
val_r2_folds = []

# éå†ä¸åŒçš„ n_estimators å€¼è¿›è¡Œè®­ç»ƒ
for n_estimators in np.arange(32, 201, 10):  # è¿™é‡Œçš„æ­¥é•¿å¯ä»¥è°ƒæ•´
    fold_train_mse = []
    fold_val_mse = []
    fold_train_r2 = []
    fold_val_r2 = []

    for fold, (train_index, val_index) in enumerate(kf.split(X_trainval), start=1):
        X_train_fold, X_val_fold = X_trainval.iloc[train_index], X_trainval.iloc[val_index]
        y_train_fold, y_val_fold = y_trainval.iloc[train_index], y_trainval.iloc[val_index]

        model = Pipeline(steps=[('preprocessor', preprocessor),
                                ('regressor', AdaBoostRegressor(
                                    n_estimators=n_estimators,
                                    learning_rate=best_learning_rate,
                                    random_state=42))])

        model.fit(X_train_fold, y_train_fold)

        # é¢„æµ‹è®­ç»ƒé›†å’ŒéªŒè¯é›†
        y_train_pred = model.predict(X_train_fold)
        y_val_pred = model.predict(X_val_fold)

        # è®¡ç®—æ¯ä¸€æŠ˜çš„MSEå’ŒRÂ²
        fold_train_mse.append(mean_squared_error(y_train_fold, y_train_pred))
        fold_val_mse.append(mean_squared_error(y_val_fold, y_val_pred))
        fold_train_r2.append(r2_score(y_train_fold, y_train_pred))
        fold_val_r2.append(r2_score(y_val_fold, y_val_pred))

    # è®¡ç®—æ¯ä¸ªè®­ç»ƒæ¬¡æ•°ä¸‹çš„å¹³å‡è¯¯å·®å’ŒRÂ²
    train_mse_folds.append(np.mean(fold_train_mse))
    val_mse_folds.append(np.mean(fold_val_mse))
    train_r2_folds.append(np.mean(fold_train_r2))
    val_r2_folds.append(np.mean(fold_val_r2))

# ç»˜åˆ¶åŒçºµè½´å­¦ä¹ æ›²çº¿ï¼ˆMSE + RÂ²ï¼‰
fig, ax1 = plt.subplots(figsize=(10, 6))

# å·¦è½´ï¼šMSE
ax1.set_xlabel('è®­ç»ƒæ¬¡æ•° (n_estimators)')
ax1.set_ylabel('å‡æ–¹è¯¯å·® (MSE)', color='tab:blue')
l1 = ax1.plot(np.arange(32, 201, 10), train_mse_folds, 'o-', color='tab:blue', label='è®­ç»ƒé›† MSE', linewidth=1.2)
l2 = ax1.plot(np.arange(32, 201, 10), val_mse_folds, 's-', color='tab:cyan', label='éªŒè¯é›† MSE', linewidth=1.2)
ax1.tick_params(axis='y', labelcolor='tab:blue')

# å³è½´ï¼šRÂ²
ax2 = ax1.twinx()
ax2.set_ylabel('å†³å®šç³»æ•° RÂ²', color='tab:red')
l3 = ax2.plot(np.arange(32, 201, 10), train_r2_folds, 'o--', color='tab:red', label='è®­ç»ƒé›† RÂ²', linewidth=1.2)
l4 = ax2.plot(np.arange(32, 201, 10), val_r2_folds, 's--', color='tab:orange', label='éªŒè¯é›† RÂ²', linewidth=1.2)
ax2.tick_params(axis='y', labelcolor='tab:red')

# åˆå¹¶å›¾ä¾‹
lines = l1 + l2 + l3 + l4
labels = [line.get_label() for line in lines]
plt.title('å­¦ä¹ æ›²çº¿ - AdaBoostï¼ˆMSE + RÂ²ï¼‰')
fig.legend(lines, labels, loc='lower center', ncol=4)
plt.grid(True)
plt.tight_layout(rect=[0, 0.05, 1, 1])
plt.show()
